{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7p0yuRNmCFcL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "df = pd.read_csv('housing.csv')\n",
        "\n",
        "\n",
        "\n",
        "df[\"income_cat\"] = pd.cut(df[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.3, stratify=df['income_cat'], random_state=42)\n",
        "train.drop('income_cat', axis=1, inplace=True)\n",
        "test.drop('income_cat',  axis=1, inplace=True)\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class Preprocessor(BaseEstimator, TransformerMixin):\n",
        "    # Train our custom preprocessors\n",
        "    numerical_columns = [\n",
        "        'longitude',\n",
        "        'latitude',\n",
        "        'housing_median_age',\n",
        "        'total_rooms',\n",
        "        'total_bedrooms',\n",
        "        'population',\n",
        "        'households',\n",
        "        'median_income',\n",
        "    ]\n",
        "    categorical_columns = [\n",
        "        'ocean_proximity'\n",
        "    ]\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "\n",
        "        # Create and fit simple imputer\n",
        "        self.imputer = SimpleImputer(strategy='median')\n",
        "        self.imputer.fit(X[self.numerical_columns])\n",
        "\n",
        "        # Create and fit Standard Scaler\n",
        "        self.scaler = StandardScaler()\n",
        "        self.scaler.fit(X[self.numerical_columns])\n",
        "\n",
        "        # Create and fit one hot encoder\n",
        "        self.onehot = OneHotEncoder(handle_unknown='ignore')\n",
        "        self.onehot.fit(X[self.categorical_columns])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "        # Apply simple imputer\n",
        "        imputed_cols = self.imputer.transform(X[self.numerical_columns])\n",
        "        onehot_cols = self.onehot.transform(X[self.categorical_columns])\n",
        "\n",
        "        # Copy the df\n",
        "        transformed_df = X.copy()\n",
        "\n",
        "        # Apply transformed columns\n",
        "        transformed_df[self.numerical_columns] = imputed_cols\n",
        "        transformed_df[self.numerical_columns] = self.scaler.transform(transformed_df[self.numerical_columns])\n",
        "\n",
        "        # Drop existing categorical columns and replace with one hot equivalent\n",
        "        transformed_df = transformed_df.drop(self.categorical_columns, axis=1)\n",
        "        transformed_df[self.onehot.get_feature_names_out()] = onehot_cols.toarray().astype(int)\n",
        "\n",
        "        return transformed_df\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "PtMbQBqyZfbx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RandomForestRegressor?"
      ],
      "metadata": {
        "id": "DW_vmuGYZgdv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WzW72GLZDbYE"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "# from xgboost.sklearn import XGBRegressor\n",
        "\n",
        "y_train = train['median_house_value']\n",
        "X_train = train.drop('median_house_value', axis=1)\n",
        "\n",
        "y_test = test['median_house_value']\n",
        "X_test = test.drop('median_house_value', axis=1)\n",
        "\n",
        "pipelines = {\n",
        "    'ridge': make_pipeline(Preprocessor(), Ridge()),\n",
        "    'rf': make_pipeline(Preprocessor(), RandomForestRegressor()),\n",
        "    'gb': make_pipeline(Preprocessor(), GradientBoostingRegressor()),\n",
        "    # 'xg': make_pipeline(Preprocessor(), XGBRegressor()),\n",
        "}\n",
        "\n",
        "\n",
        "grid = {\n",
        "    'ridge':{'ridge__alpha':[0.05, 0.25, 0.5, 1.0]},\n",
        "    'rf':{\n",
        "        'randomforestregressor__n_estimators':[50,100,150],\n",
        "        'randomforestregressor__max_depth':[5,6,7,None]\n",
        "    },\n",
        "    'gb':{\n",
        "        'gradientboostingregressor__n_estimators':[50,100,150],\n",
        "        'gradientboostingregressor__max_depth':[5,6,7, None]\n",
        "    },\n",
        "    # 'xg':{\n",
        "    #    'xgbregressor__n_estimators':[50,100,150],\n",
        "    #    'xgbregressor__max_depth':[5,6,7,None]\n",
        "    # }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tiJS9yKDqoW",
        "outputId": "cd3989b8-802a-461f-a0ae-b104a9de3e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ridge\n",
            "ridge 4.012654983000175\n",
            "rf\n",
            "rf 560.2899177019999\n",
            "gb\n",
            "gb 856.9477929930001\n",
            "1421.2571128270001\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import time\n",
        "total_start = time.monotonic()\n",
        "fit_models = {}\n",
        "for algo, pipeline in pipelines.items():\n",
        "    try:\n",
        "        print(algo)\n",
        "        start = time.monotonic()\n",
        "        model = GridSearchCV(pipeline, grid[algo], n_jobs=-1, cv=10, scoring='r2')\n",
        "        model.fit(X_train, y_train)\n",
        "        fit_models[algo] = model\n",
        "        end = time.monotonic()\n",
        "        print(algo, end-start)\n",
        "    except Exception as e:\n",
        "        print(f'Model {algo} had an error {e}')\n",
        "\n",
        "end = time.monotonic()\n",
        "print(end-total_start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fit_models['ridge'].best_estimator_[1], fit_models['ridge'].best_score_)\n",
        "print(fit_models['rf'].best_estimator_[1], fit_models['rf'].best_score_)\n",
        "print(fit_models['gb'].best_estimator_[1], fit_models['gb'].best_score_)\n",
        "\n"
      ],
      "metadata": {
        "id": "nluBa0wmNXe_",
        "outputId": "7f85a8ce-7f16-4e96-825c-a30cda1ef173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge(alpha=0.25) 0.6402134848239246\n",
            "RandomForestRegressor(n_estimators=150) 0.8171769026724179\n",
            "GradientBoostingRegressor(max_depth=7, n_estimators=150) 0.8294081050646742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit_models['rf'].best_estimator_[1].feature_names_in_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdO7EdOZi-K-",
        "outputId": "079a5fe2-35fb-46ae-ad4d-14511c3dde4b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
              "       'total_bedrooms', 'population', 'households', 'median_income',\n",
              "       'ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND',\n",
              "       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',\n",
              "       'ocean_proximity_NEAR OCEAN'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "YZTTWJt5nhkm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uLe89asn40e",
        "outputId": "b195c439-5eb6-4ae9-fe3f-516b5382860d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gradientboostingregressor__max_depth': 7,\n",
              " 'gradientboostingregressor__n_estimators': 150}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "XZ_Jh6-KogfW",
        "outputId": "56c39a1c-18e8-4e1d-fdde-e88653719fdb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"GridSearchCV(cv=10,\\n             estimator=Pipeline(steps=[('preprocessor', Preprocessor()),\\n                                       ('gradientboostingregressor',\\n                                        GradientBoostingRegressor())]),\\n             n_jobs=-1,\\n             param_grid={'gradientboostingregressor__max_depth': [5, 6, 7,\\n                                                                  None],\\n                         'gradientboostingregressor__n_estimators': [50, 100,\\n                                                                     150]},\\n             scoring='r2')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = fit_models['rf'].best_estimator_[1]\n",
        "for score, name in sorted(zip(classifier.feature_importances_, classifier.feature_names_in_), reverse=True):\n",
        "    print(round(score, 2), name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COrCoKLsjW90",
        "outputId": "1ac01a43-486d-4453-a1a2-4d0081e62545"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.48 median_income\n",
            "0.15 ocean_proximity_INLAND\n",
            "0.11 longitude\n",
            "0.1 latitude\n",
            "0.05 housing_median_age\n",
            "0.03 population\n",
            "0.03 total_rooms\n",
            "0.02 total_bedrooms\n",
            "0.02 households\n",
            "0.01 ocean_proximity_NEAR OCEAN\n",
            "0.0 ocean_proximity_<1H OCEAN\n",
            "0.0 ocean_proximity_NEAR BAY\n",
            "0.0 ocean_proximity_ISLAND\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit_models['rf'].best_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6ePrC4XjrgJ",
        "outputId": "23d17a85-efbb-4220-f969-144fdb1b36e3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8171769026724179"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XWmnRl7o2Xw",
        "outputId": "4068f6d1-1eb5-4d49-843d-c67089d0184c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Using cached mlflow-2.12.1-py3-none-any.whl (20.2 MB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Using cached alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.1)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Using cached docker-7.0.0-py3-none-any.whl (147 kB)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4)\n",
            "Collecting gitpython<4,>=3.1.9 (from mlflow)\n",
            "  Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Using cached graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.6)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.25.2)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.10/dist-packages (from mlflow) (24.0)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.3)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.20.3)\n",
            "Requirement already satisfied: pyarrow<16,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (14.0.2)\n",
            "Requirement already satisfied: pytz<2025 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2023.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.0.1)\n",
            "Collecting querystring-parser<2 (from mlflow)\n",
            "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.11.4)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.29)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.5.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.3)\n",
            "Collecting gunicorn<22 (from mlflow)\n",
            "  Using cached gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Using cached Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.11.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.0.7)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow)\n",
            "  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Using cached graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Collecting aniso8601<10,>=8 (from graphene<4->mlflow)\n",
            "  Using cached aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow)\n",
            "  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: aniso8601, smmap, querystring-parser, Mako, gunicorn, graphql-core, graphql-relay, gitdb, docker, alembic, graphene, gitpython, mlflow\n",
            "Successfully installed Mako-1.3.3 alembic-1.13.1 aniso8601-9.0.1 docker-7.0.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-21.2.0 mlflow-2.12.1 querystring-parser-1.2.4 smmap-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "MLFLOW_TRACKING_URI=\"https://dagshub.com/mkzia/house_models.mlflow\"\n",
        "os.environ['MLFLOW_TRACKING_USERNAME']='mkzia'\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD']='bbbc0c41d162cdc2a38ab1aacdc1a8ff6987d08f'\n",
        "\n",
        "\n",
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "# Set our tracking server uri for logging\n",
        "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
        "\n",
        "# Create a new MLflow Experiment\n",
        "mlflow.set_experiment(\"median_house_pricing\")\n",
        "\n",
        "# Start an MLflow run\n",
        "for algo, model in fit_models.items():\n",
        "  score = model.best_score_\n",
        "  params = model.best_params_\n",
        "  with mlflow.start_run():\n",
        "      # Log the hyperparameters\n",
        "      mlflow.log_params(params)\n",
        "\n",
        "      # Log metrics\n",
        "      mlflow.log_metric(\"r2\", score)\n",
        "      # Infer the model signature\n",
        "      signature = infer_signature(X_train, model.best_estimator_.predict(X_train))\n",
        "\n",
        "      # Log the model\n",
        "      model_info = mlflow.sklearn.log_model(\n",
        "          sk_model=model,\n",
        "          artifact_path=\"housing_model\",\n",
        "          signature=signature,\n",
        "          input_example=X_train,\n",
        "          registered_model_name=algo,\n",
        "      )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa9QVElEiU2I",
        "outputId": "1eb56657-6329-49d1-bbc7-3990c81e29d7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Registered model 'ridge' already exists. Creating a new version of this model...\n",
            "2024/05/02 23:32:11 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ridge, version 2\n",
            "Created version '2' of model 'ridge'.\n",
            "Successfully registered model 'rf'.\n",
            "2024/05/02 23:32:43 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: rf, version 1\n",
            "Created version '1' of model 'rf'.\n",
            "Successfully registered model 'gb'.\n",
            "2024/05/02 23:33:09 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: gb, version 1\n",
            "Created version '1' of model 'gb'.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}